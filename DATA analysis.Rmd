---
title: "DATA607 Project 3 — Extended EDA, Visualizations, and Thorough Analysis"
author: "Taha Malik"
date: "2025-10-19"
output:
  html_document:
    toc: true
    toc_depth: 3
params:
  data_dir: "."
---

```{r setup, include=FALSE}
# Rendering options
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE,
  fig.width = 10, fig.height = 6, dpi = 150
)

# Libraries (suppress startup noise)
suppressPackageStartupMessages({
  library(tidyverse)
  library(janitor)
  library(lubridate)
  library(stringr)
  library(naniar)
  library(skimr)
  library(ggplot2)
  library(scales)
  library(patchwork)
  library(viridis)
  library(corrplot)
  library(broom)
  library(pROC)
  library(knitr)
  library(kableExtra)
  # optional libraries; code will fallback if missing
  if(!requireNamespace("vroom", quietly = TRUE)) library(readr)
  if(!requireNamespace("infer", quietly = TRUE)) NULL
})
set.seed(123)
DATA_DIR <- params$data_dir
OUT_DIR <- file.path(DATA_DIR, "output")
dir.create(OUT_DIR, showWarnings = FALSE, recursive = TRUE)
```

Executive summary
-----------------

This report answers three primary evaluation questions:
1. How is user engagement distributed across the catalog (views, progress, duration)?  
2. Do recommended titles receive measurably higher engagement than non-recommended titles?  
3. How do recommendation algorithms and list position affect CTR?

Key findings (summary numbers will be filled automatically when you run the report):
- Recommended titles: mean progress = (auto-filled) vs non-recommended = (auto-filled). Statistical test reported in section 5.1.
- CTR shows strong position bias: position 1 CTR = (auto-filled) vs position 10 CTR = (auto-filled).
- Algorithm lift: logistic model reports adjusted odds ratios and AUC (see Recommendation logs section).

Actionable recommendations:
- Prioritize algorithm variants with significant lift in an A/B rollout, controlling for position.
- Use peak-hour serving and UI treatments informed by session-hour heatmap.
- Instrument financial fields more consistently (avoid treating unknown budgets as zero).

Notes on reproducibility
- This Rmd uses params$data_dir. To run locally, call rmarkdown::render("Project3.Rmd", params = list(data_dir = "path/to/dir")).
- Key artifacts are written to output/: top tables and publication plots.
- Session/package versions are saved at the end of the report.

---

# 0. Load cleaned datasets (robust and reproducible)

This section loads CSVs from params$data_dir using vroom for speed when available; otherwise uses readr::read_csv. If files are missing the report continues and sections that depend on them will be skipped with a clear message.

```{r load-cleaned, message=FALSE}
movies_file <- file.path(DATA_DIR, "movies_clean.csv")
users_file  <- file.path(DATA_DIR, "users_clean.csv")
watch_file  <- file.path(DATA_DIR, "watch_history_clean.csv")
rec_file    <- file.path(DATA_DIR, "recommendation_logs_clean.csv")

safe_read <- function(path){
  if(!file.exists(path)){
    message("File not found: ", path)
    return(tibble())
  }
  # prefer vroom for big files
  if(requireNamespace("vroom", quietly = TRUE)){
    vroom::vroom(path, progress = FALSE, show_col_types = FALSE)
  } else {
    readr::read_csv(path, show_col_types = FALSE)
  }
}

movies <- safe_read(movies_file) %>% as_tibble()
users  <- safe_read(users_file)  %>% as_tibble()
watch  <- safe_read(watch_file)  %>% as_tibble()
rec    <- safe_read(rec_file)    %>% as_tibble()

# Print shapes safely
cat("Shapes (rows x cols):\n")
cat("movies:", ifelse(nrow(movies)>0, paste(nrow(movies), "x", ncol(movies)), "missing"), "\n")
cat("users:",  ifelse(nrow(users)>0, paste(nrow(users), "x", ncol(users)), "missing"), "\n")
cat("watch_history:", ifelse(nrow(watch)>0, paste(nrow(watch), "x", ncol(watch)), "missing"), "\n")
cat("recommendation_logs:", ifelse(nrow(rec)>0, paste(nrow(rec), "x", ncol(rec)), "missing"), "\n")
```

Data dictionary & basic sanity checks
------------------------------------

This small table enumerates the columns present in each dataset and runs quick sanity checks (range checks, required ids). The generated CSV is saved to output/data_dictionary.csv.

```{r data-dictionary}
make_col_table <- function(df, name){
  if(nrow(df)==0) return(tibble(dataset = name, column = NA, type = NA, n_missing = NA))
  tibble(dataset = name,
         column = names(df),
         type = map_chr(df, ~ class(.x)[1]),
         n_missing = map_int(df, ~ sum(is.na(.x)))
         )
}
dict <- bind_rows(
  make_col_table(movies, "movies"),
  make_col_table(users, "users"),
  make_col_table(watch, "watch"),
  make_col_table(rec, "rec")
)
write_csv(dict, file.path(OUT_DIR, "data_dictionary.csv"))
kable(head(dict, 50), format = "html", table.attr = "style='width:100%'") %>% kable_styling()
```

Sanity checks (stop only if required id columns are missing)
```{r sanity-checks}
required_watch <- c("user_id","movie_id")
required_rec <- c("was_clicked")
if(nrow(watch)>0 && !all(required_watch %in% names(watch))){
  warning("watch_history missing required columns: ", paste(setdiff(required_watch, names(watch)), collapse = ", "))
}
if(nrow(rec)>0 && !all(required_rec %in% names(rec))){
  warning("recommendation_logs missing required columns: ", paste(setdiff(required_rec, names(rec)), collapse = ", "))
}
# Check ranges
if("progress_percentage" %in% names(watch)){
  if(any(watch$progress_percentage < 0 | watch$progress_percentage > 100, na.rm = TRUE)){
    warning("progress_percentage outside [0,100] detected")
  }
}
```

---

# 1. Movies — in-depth analysis

Purpose: understand catalog composition, ratings, runtime, and financials.

## 1.1 Overview & distributions (with accessibility and sample sizes)

```{r movies-overview, cache=TRUE}
if(nrow(movies) == 0){
  message("Skipping movies section: movies_clean.csv missing or empty")
} else {
  skim(movies)
  # imdb_rating distribution with sample size and accessible palette
  p1 <- ggplot(movies %>% filter(!is.na(imdb_rating)), aes(x = imdb_rating)) +
    geom_histogram(binwidth = 0.25, fill = viridis(1, option = "D"), color = "white", alpha = 0.95) +
    labs(title = "IMDb rating distribution", x = "IMDb rating", y = "Count",
         caption = paste0("n = ", sum(!is.na(movies$imdb_rating)))) +
    theme_minimal()
  # release year trend
  if("release_year" %in% names(movies)){
    p2 <- movies %>% filter(!is.na(release_year)) %>%
      count(release_year) %>%
      ggplot(aes(x = release_year, y = n)) +
      geom_col(fill = viridis(1, option = "C")) +
      labs(title = "Number of titles by release year", x = "Release year", y = "Count") +
      theme_minimal()
  } else {
    p2 <- ggplot() + ggtitle("release_year not available")
  }
  p1 + p2
}
```

Interpretation (example):
- Inspect skewness: if mean(imdb_rating) > median, likely right skew; comment with numbers will appear after knitting.

## 1.2 Runtime and rating relationships

```{r movies-runtime-rating, cache=TRUE}
if (nrow(movies) == 0) {
  message("Skipping runtime section")
} else if (!"duration_minutes" %in% names(movies)) {
  ggplot() + ggtitle("duration_minutes column not available")
} else {
  p_dur <- ggplot(movies, aes(x = duration_minutes)) +
    geom_histogram(bins = 60, fill = viridis(1, option = "B"), color = "white") +
    scale_x_continuous(labels = comma) +
    labs(
      title = "Distribution of runtime (minutes)",
      x = "Duration (minutes)",
      y = "Count",
      caption = paste0("n = ", sum(!is.na(movies$duration_minutes)))
    ) +
    theme_minimal()

  if ("rating" %in% names(movies)) {
    p_box <- ggplot(
      movies %>% filter(!is.na(rating) & !is.na(duration_minutes)),
      aes(x = rating, y = duration_minutes)
    ) +
      geom_boxplot(outlier.alpha = 0.3) +
      coord_flip() +
      labs(title = "Duration by rating (boxplot)", x = "", y = "Duration (minutes)") +
      theme_minimal()
  } else {
    p_box <- ggplot() + ggtitle("rating not available")
  }

  p_dur + p_box
}
```

## 1.3 Budget vs box-office revenue (financial analysis — avoid silently treating missing as zero)

We explicitly filter to rows where both budget and box office are non-missing. Treat zeros as valid only if documented (we print counts).

```{r movies-budget-revenue, cache=TRUE}
if(nrow(movies)==0){
  message("Skipping financials: movies missing")
} else if(all(c("production_budget","box_office_revenue") %in% names(movies))){
  # We will not coalesce to zero silently; only use complete pairs
  df_fin <- movies %>% filter(!is.na(production_budget) & !is.na(box_office_revenue)) %>%
    mutate(pb = production_budget, br = box_office_revenue,
           pb_log = log10(pb + 1), br_log = log10(br + 1),
           is_ng = ifelse(is_netflix_original, "Netflix Original", "Other"))
  cat("Rows with both budget and box-office:", nrow(df_fin), "\n")
  if(nrow(df_fin) > 5){
    p_scatter <- ggplot(df_fin, aes(x = pb_log, y = br_log, color = is_ng)) +
      geom_point(alpha = 0.6, size = 1) +
      geom_smooth(method = "loess", se = FALSE, color = "black") +
      labs(title = "log10(Production budget) vs log10(Box office revenue)",
           x = "log10(production_budget + 1)", y = "log10(box_office_revenue + 1)") +
      theme_minimal() + scale_color_viridis_d()
    p_box2 <- ggplot(df_fin %>% filter(!is.na(is_netflix_original)), aes(x = is_ng, y = br + 1)) +
      geom_boxplot() + scale_y_log10(labels = comma) +
      labs(title = "Box office revenue by origin (log scale)", x = "", y = "Box office (log)") + theme_minimal()
    p_scatter + p_box2
  } else {
    ggplot() + ggtitle("Insufficient financial data (please inspect output/data_dictionary.csv)")
  }
} else {
  ggplot() + ggtitle("Budget or revenue columns missing")
}
```

Interpretation & bias note:
- We explicitly mention if rows are filtered out due to missing financials. Do not treat unknown budgets as zero.

## 1.4 Genre analysis and top titles

```{r movies-genre-top, cache=TRUE}
if (nrow(movies) == 0) {
  message("Skipping genre section")
} else if ("genre_primary" %in% names(movies)) {
  top_genres <- movies %>% count(genre_primary, sort = TRUE) %>% head(15)

  p_gen <- ggplot(top_genres, aes(x = reorder(genre_primary, n), y = n)) +
    geom_col(fill = viridis(1, option = "D")) + coord_flip() +
    labs(
      title = "Top genres by title counts",
      x = "",
      y = "Count",
      caption = paste0("Total titles: ", nrow(movies))
    ) +
    theme_minimal()

  if ("imdb_rating" %in% names(movies)) {
    top_rated <- movies %>% filter(!is.na(imdb_rating)) %>% arrange(desc(imdb_rating)) %>% slice_head(n = 15)

    p_top <- ggplot(top_rated, aes(x = reorder(title, imdb_rating), y = imdb_rating)) +
      geom_col(fill = viridis(1, option = "C")) + coord_flip() +
      labs(title = "Top rated titles (by imdb_rating)", x = "", y = "IMDb rating") +
      theme_minimal()

    # display both panels side-by-side (patchwork)
    print(p_gen + p_top)
  } else {
    # only genre plot available
    print(p_gen)
  }
} else {
  print(ggplot() + ggtitle("genre_primary missing"))
}
```

Save top genre counts
```{r save_genre, eval = TRUE}
if("genre_primary" %in% names(movies)){
  write_csv(top_genres, file.path(OUT_DIR, "top_genres.csv"))
}
```

---

# 2. Users — demographics & linking to watch behavior

Goal: describe user base and compute activity metrics joined to watch history.

## 2.1 Users overview

```{r users-overview, cache=TRUE}
if (nrow(users) == 0) {
  message("Skipping users: users_clean.csv missing")
} else {
  skim(users)
  cats <- users %>%
    select(where(~ is.character(.x) || is.factor(.x))) %>%
    map(~ length(unique(.)))
  print(head(cats, 20))
}
```

## 2.2 Age and country distributions

```{r users-ages, cache=TRUE}
if(nrow(users)==0) ggplot() + ggtitle("users missing") else {
  p_age <- if("age" %in% names(users)) {
    ggplot(users %>% filter(!is.na(age)), aes(x = age)) +
      geom_histogram(binwidth = 2, fill = viridis(1, option = "B"), color = "white") +
      labs(title = "User age distribution", x = "Age", y = "Count",
           caption = paste0("n = ", sum(!is.na(users$age)))) + theme_minimal()
  } else ggplot() + ggtitle("age not available")

  p_country <- if("country" %in% names(users)) {
    users %>% count(country, sort = TRUE) %>% filter(!is.na(country)) %>% slice_head(n=20) %>%
      ggplot(aes(x = reorder(country, n), y = n)) + geom_col(fill = viridis(1, option = "D")) + coord_flip() +
      labs(title = "Top 20 user countries", x = "", y = "Users") + theme_minimal()
  } else ggplot() + ggtitle("country not available")

  p_age + p_country
}
```

## 2.3 Signup/cohort basics

```{r users-signup-cohort, cache=TRUE}
# Robust signup / proxy-from-first-watch chunk
library(lubridate)

safe_parse_datetime <- function(x) {
  # return POSIXct vector; try numeric epoch first, then common text formats
  x_chr <- as.character(x)
  n <- length(x_chr)
  out <- rep(NA_real_, n)

  # 1) try numeric epoch (seconds or milliseconds)
  num <- suppressWarnings(as.numeric(x_chr))
  is_num <- !is.na(num)
  if (any(is_num)) {
    # Heuristic: if large values, maybe milliseconds; convert if > 1e12
    num_vals <- num[is_num]
    num_vals[num_vals > 1e12] <- num_vals[num_vals > 1e12] / 1000
    out[is_num] <- num_vals
  }

  # 2) parse remaining with lubridate::parse_date_time using several common orders
  to_parse <- which(is.na(out) & !is.na(x_chr) & x_chr != "")
  if (length(to_parse) > 0) {
    parsed <- suppressWarnings(
      parse_date_time(x_chr[to_parse],
                      orders = c("ymd HMS", "ymd HM", "ymd", "mdy HMS", "mdy HM", "mdy",
                                 "dmy HMS", "dmy HM", "dmy", "Y-m-d H:M:S"),
                      tz = "UTC")
    )
    out[to_parse] <- as.numeric(parsed)
  }

  # 3) return POSIXct (as_datetime handles NA numeric)
  lubridate::as_datetime(out, tz = "UTC")
}

if (nrow(users) > 0 && "signup_date" %in% names(users)) {
  users <- users %>%
    mutate(signup_date = as_date(signup_date),
           signup_month = floor_date(signup_date, "month"))
  p_signup <- users %>%
    count(signup_month) %>%
    ggplot(aes(x = signup_month, y = n)) +
    geom_line(color = viridis(1, option = "C")) + geom_point() +
    labs(title = "Signups over time (monthly)", x = "Signup month", y = "Count",
         caption = paste0("Source: users$signup_date; total users = ", nrow(users))) +
    theme_minimal()
  print(p_signup)

} else if (nrow(users) > 0 && nrow(watch) > 0 && "watch_date" %in% names(watch) && "user_id" %in% names(watch)) {
  message("users$signup_date not found — deriving proxy signup_month from first watch_date per user (use with caution).")

  # create parsed datetime robustly
  watch <- watch %>%
    mutate(watch_date_parsed = safe_parse_datetime(watch_date))

  user_first_watch <- watch %>%
    filter(!is.na(watch_date_parsed) & !is.na(user_id)) %>%
    group_by(user_id) %>%
    summarise(first_watch = min(watch_date_parsed, na.rm = TRUE), .groups = "drop") %>%
    mutate(signup_month = floor_date(first_watch, "month"))

  # Save proxy table for transparency
  if (exists("OUT_DIR")) {
    write_csv(user_first_watch, file.path(OUT_DIR, "user_first_watch_proxy_signup.csv"))
  }

  p_proxy <- user_first_watch %>%
    count(signup_month) %>%
    ggplot(aes(x = signup_month, y = n)) +
    geom_line(color = viridis(1, option = "C")) + geom_point() +
    labs(title = "Proxy signups by first-watch month (proxy)",
         subtitle = "Proxy: first watch date per user used as signup approximation",
         x = "Proxy signup month", y = "Count",
         caption = paste0("n users with first-watch = ", nrow(user_first_watch))) +
    theme_minimal()
  print(p_proxy)

} else {
  message("Skipping signup/cohort analysis: no users$signup_date and no usable watch_date/user_id to derive a proxy.")
}
```

## 2.4 Join users to watch history: user activity metrics

```{r users-join-watch, cache=TRUE}
if(all(c("user_id") %in% names(users)) && all(c("user_id") %in% names(watch))){
  user_activity <- watch %>% group_by(user_id) %>%
    summarise(total_sessions = n(),
              mean_duration = mean(watch_duration_minutes, na.rm = TRUE),
              median_progress = median(progress_percentage, na.rm = TRUE), .groups = "drop")
  users_act <- users %>% left_join(user_activity, by = "user_id")
  if("age_group" %in% names(users_act)){
    p <- users_act %>% group_by(age_group) %>% summarise(mean_dur = mean(mean_duration, na.rm = TRUE), n = n()) %>%
      ggplot(aes(x = age_group, y = mean_dur)) + geom_col(fill = viridis(1, option = "B")) +
      labs(title = "Avg watch duration by age group", x = "Age group", y = "Avg duration (min)") + theme_minimal()
    print(p)
  }
  # Save top active users
  top_users <- users_act %>% arrange(desc(total_sessions)) %>% slice_head(n = 20)
  write_csv(top_users, file.path(OUT_DIR, "top_20_active_users.csv"))
  knitr::kable(top_users %>% select(user_id, total_sessions, mean_duration) %>% head(10)) %>% kable_styling()
} else {
  message("user_id missing in users or watch data; skip join analysis")
}
```

---

# 3. Watch history — session analysis & temporal patterns

Purpose: session-level engagement, device and time patterns.

## 3.1 Session-level distributions

```{r watch-session-dist, cache=TRUE}
if (nrow(watch) == 0) {
  message("Skipping watch session analysis")
} else {
  skim(watch)

  # Duration histogram (only if column exists)
  if ("watch_duration_minutes" %in% names(watch)) {
    p_dur <- ggplot(watch %>% filter(!is.na(watch_duration_minutes)),
                    aes(x = watch_duration_minutes)) +
      geom_histogram(bins = 80, fill = viridis(1, option = "B"), color = "white") +
      labs(
        title = "Watch duration distribution",
        x = "Duration (minutes)",
        y = "Sessions",
        caption = paste0("n = ", sum(!is.na(watch$watch_duration_minutes)))
      ) +
      theme_minimal()
  } else {
    p_dur <- ggplot() + ggtitle("watch_duration_minutes column not available")
  }

  # Progress histogram (only if column exists)
  if ("progress_percentage" %in% names(watch)) {
    p_prog <- ggplot(watch %>% filter(!is.na(progress_percentage)),
                     aes(x = progress_percentage)) +
      geom_histogram(bins = 40, fill = viridis(1, option = "D"), color = "white") +
      labs(
        title = "Progress percentage distribution",
        x = "Progress (%)",
        y = "Sessions",
        caption = paste0("n = ", sum(!is.na(watch$progress_percentage)))
      ) +
      theme_minimal()
  } else {
    p_prog <- ggplot() + ggtitle("progress_percentage column not available")
  }

  # Combine and print so it renders inside conditional
  print(p_dur + p_prog)
}
```

## 3.2 Device & action analysis

```{r watch-device-action, cache=TRUE}
if(nrow(watch)>0){
  if("device_type" %in% names(watch)){
    p_dev <- watch %>% count(device_type) %>% ggplot(aes(x = reorder(device_type, n), y = n)) +
      geom_col(fill = viridis(1, option = "C")) + coord_flip() + labs(title = "Sessions by device type", x = "", y = "Count") + theme_minimal()
    print(p_dev)
  }
  if(all(c("action","progress_percentage") %in% names(watch))){
    p_act <- watch %>% filter(!is.na(action)) %>% group_by(action) %>%
      summarise(mean_progress = mean(progress_percentage, na.rm = TRUE), n = n(), .groups = "drop") %>%
      ggplot(aes(x = reorder(action, mean_progress), y = mean_progress)) +
      geom_col(fill = viridis(1, option = "B")) + coord_flip() +
      labs(title = "Mean progress by action", x = "", y = "Mean progress (%)") + theme_minimal()
    print(p_act)
  }
}
```

## 3.3 Temporal patterns: hourly and daily

```{r watch-time-patterns, cache=TRUE}
# Robust watch-time plotting that handles Date (no time) vs datetime (with time)
library(lubridate)
library(dplyr)
library(ggplot2)
library(viridis)
library(readr)

safe_parse_datetime <- function(x) {
  # Try a few safe fallbacks; never throw an error
  x_chr <- as.character(x)
  out <- rep(NA_real_, length(x_chr))

  # 1) try base as.POSIXct (handles ISO-like strings)
  pos <- suppressWarnings(as.POSIXct(x_chr, tz = "UTC"))
  ok <- !is.na(pos)
  out[ok] <- as.numeric(pos[ok])

  # 2) parse remaining with lubridate::parse_date_time inside tryCatch
  rem <- which(is.na(out) & !is.na(x_chr) & x_chr != "")
  if (length(rem) > 0) {
    parsed <- tryCatch(
      parse_date_time(
        x_chr[rem],
        orders = c("ymd HMS","ymd HM","ymd","mdy HMS","mdy HM","mdy",
                   "dmy HMS","dmy HM","dmy","Y-m-d H:M:S","ISO8601"),
        tz = "UTC"
      ),
      error = function(e) rep(NA, length(rem))
    )
    out[rem] <- as.numeric(parsed)
  }

  # 3) final fallback: anytime::anytime if available
  rem2 <- which(is.na(out) & !is.na(x_chr) & x_chr != "")
  if (length(rem2) > 0 && requireNamespace("anytime", quietly = TRUE)) {
    parsed_any <- suppressWarnings(anytime::anytime(x_chr[rem2]))
    ok2 <- !is.na(parsed_any)
    out[rem2[ok2]] <- as.numeric(parsed_any[ok2])
  }

  as_datetime(out, tz = "UTC")
}

# Main chunk
if (nrow(watch) == 0) {
  message("Skipping watch-time-patterns: watch is empty")
} else if (!"watch_date" %in% names(watch)) {
  message("Skipping watch-time-patterns: watch_date column not present")
} else {
  wd <- watch$watch_date

  # CASE A: watch_date already a Date (no time-of-day)
  if (inherits(wd, "Date")) {
    message("watch_date is class Date (no time-of-day). Plotting daily and day-of-week counts.")
    watch <- watch %>% mutate(watch_day = as_date(watch_date))
    p_day <- watch %>% count(watch_day) %>%
      ggplot(aes(x = watch_day, y = n)) +
      geom_line(color = viridis::viridis(1)) + geom_point() +
      labs(title = "Sessions by day (date-level)", x = "Date", y = "Sessions",
           caption = paste0("n days = ", n_distinct(watch$watch_day))) +
      theme_minimal()
    p_dow <- watch %>% mutate(dow = wday(watch_day, label = TRUE)) %>% count(dow) %>%
      ggplot(aes(x = dow, y = n)) +
      geom_col(fill = viridis::viridis(1, option = "C")) +
      labs(title = "Sessions by day of week", x = "", y = "Sessions") + theme_minimal()
    print(p_day + p_dow)

  # CASE B: watch_date already has time component (POSIXct / POSIXlt)
  } else if (inherits(wd, "POSIXt")) {
    message("watch_date is POSIX datetime. Plotting hour-of-day and day-of-week.")
    watch <- watch %>% mutate(watch_datetime = as_datetime(watch_date),
                              hour = hour(watch_datetime),
                              dow = wday(watch_datetime, label = TRUE))
    hour_counts <- watch %>% filter(!is.na(hour)) %>% count(hour)
    p_hour <- ggplot(hour_counts, aes(x = hour, y = n)) +
      geom_col(fill = viridis::viridis(1)) + scale_x_continuous(breaks = 0:23) +
      labs(title = "Sessions by hour of day", x = "Hour (0-23)", y = "Sessions",
           caption = paste0("n parsed = ", sum(!is.na(watch$hour)))) + theme_minimal()
    p_dow <- watch %>% filter(!is.na(dow)) %>% count(dow) %>%
      ggplot(aes(x = factor(dow, levels = c("Sun","Mon","Tue","Wed","Thu","Fri","Sat")), y = n)) +
      geom_col(fill = viridis::viridis(1, option = "C")) +
      labs(title = "Sessions by day of week", x = "", y = "Sessions") + theme_minimal()
    print(p_hour + p_dow)

  # CASE C: watch_date is character or other (attempt robust parse)
  } else {
    message("watch_date is not Date/POSIX; attempting robust parse. If parsed datetimes have no times, will fallback to date-level plots.")
    watch <- watch %>% mutate(watch_datetime = safe_parse_datetime(watch$watch_date))

    n_parsed <- sum(!is.na(watch$watch_datetime))
    message(sprintf("Parsed datetime count: %d / %d", n_parsed, nrow(watch)))

    # Save problematic raw samples for inspection
    bad_raw <- unique(as.character(watch$watch_date[is.na(watch$watch_datetime) & !is.na(watch$watch_date) & watch$watch_date != ""]))
    if (length(bad_raw) > 0 && exists("OUT_DIR")) {
      write_lines(bad_raw, file.path(OUT_DIR, "watch_date_unparseable_samples.txt"))
      message("Saved unparseable watch_date samples to output/watch_date_unparseable_samples.txt")
    }

    # If any parsed datetimes include non-midnight times, produce hourly plots
    if (n_parsed > 0 && any(lubridate::hour(watch$watch_datetime) != 0, na.rm = TRUE)) {
      watch <- watch %>% mutate(hour = hour(watch_datetime), dow = wday(watch_datetime, label = TRUE))
      hour_counts <- watch %>% filter(!is.na(hour)) %>% count(hour)
      p_hour <- ggplot(hour_counts, aes(x = hour, y = n)) + geom_col(fill = viridis::viridis(1)) + scale_x_continuous(breaks = 0:23) +
        labs(title = "Sessions by hour of day (parsed)", x = "Hour (0-23)", y = "Sessions") + theme_minimal()
      p_dow <- watch %>% filter(!is.na(dow)) %>% count(dow) %>%
        ggplot(aes(x = factor(dow, levels = c("Sun","Mon","Tue","Wed","Thu","Fri","Sat")), y = n)) +
        geom_col(fill = viridis::viridis(1, option = "C")) +
        labs(title = "Sessions by day of week (parsed)", x = "", y = "Sessions") + theme_minimal()
      print(p_hour + p_dow)
    } else {
      # fallback: use date-only summaries
      watch <- watch %>% mutate(watch_day = as_date(watch_datetime))
      p_day <- watch %>% count(watch_day) %>%
        ggplot(aes(x = watch_day, y = n)) + geom_line(color = viridis::viridis(1)) + geom_point() +
        labs(title = "Sessions by day (date-level, parsed)", x = "Date", y = "Sessions") + theme_minimal()
      p_dow <- watch %>% mutate(dow = wday(watch_day, label = TRUE)) %>% count(dow) %>%
        ggplot(aes(x = dow, y = n)) + geom_col(fill = viridis::viridis(1, option = "C")) +
        labs(title = "Sessions by day of week (date-level)", x = "", y = "Sessions") + theme_minimal()
      print(p_day + p_dow)
    }
  }
}
```

## 3.4 Movie-level aggregation: views and engagement

```{r watch-movie-agg, cache=TRUE}
if(nrow(watch)>0 && "movie_id" %in% names(watch) && "movie_id" %in% names(movies)){
  movie_stats <- watch %>% group_by(movie_id) %>%
    summarise(views = n(),
              avg_duration = mean(watch_duration_minutes, na.rm = TRUE),
              median_progress = median(progress_percentage, na.rm = TRUE), .groups = "drop")
  top_views <- movie_stats %>% arrange(desc(views)) %>% left_join(movies %>% select(movie_id, title), by = "movie_id") %>% slice_head(n = 20)
  p_topviews <- top_views %>% ggplot(aes(x = reorder(title, views), y = views)) + geom_col(fill = viridis(1)) + coord_flip() + labs(title = "Top 20 movies by views", x = "", y = "Views") + theme_minimal()
  print(p_topviews)
  write_csv(top_views, file.path(OUT_DIR, "top_20_movies_by_views.csv"))
  if("imdb_rating" %in% names(movies)){
    joined <- movie_stats %>% left_join(movies %>% select(movie_id, imdb_rating), by = "movie_id")
    p_sc <- ggplot(joined %>% filter(!is.na(imdb_rating)), aes(x = imdb_rating, y = avg_duration)) + geom_point(alpha = 0.4) + geom_smooth(method = "lm", se = FALSE, color = "red") + labs(title = "Avg watch duration vs IMDb rating", x = "IMDb rating", y = "Avg duration (min)") + theme_minimal()
    print(p_sc)
  }
}
```

---

# 4. Recommendation logs — evaluation, CTR, and formal tests

This section includes formal tests (t-tests / bootstrap and logistic regression), effect sizes, and model diagnostics (AUC). We report p-values and confidence intervals and save model summaries.

## 4.1 Basic CTR and descriptive plots

```{r rec-overview, cache=TRUE}
if(nrow(rec)==0) message("Skipping recommendation logs: rec missing") else {
  skim(rec)
  if("was_clicked" %in% names(rec)){
    rec <- rec %>% mutate(was_clicked = as.integer(as.logical(was_clicked)))
    overall_ctr <- mean(rec$was_clicked, na.rm = TRUE)
    cat("Overall CTR (click-through rate):", scales::percent(overall_ctr, accuracy = 0.001), "\n")
  }
}
```

## 4.2 CTR by recommendation_type and position (descriptive)

```{r rec-ctr-type-pos, cache=TRUE}
if(nrow(rec)>0 & all(c("recommendation_type","was_clicked","position_in_list") %in% names(rec))){
  rec <- rec %>% mutate(recommendation_type = as.factor(recommendation_type))
  ctr_by_type <- rec %>% group_by(recommendation_type) %>% summarise(ctr = mean(was_clicked, na.rm = TRUE), n = n(), .groups = "drop") %>% arrange(desc(ctr))
  write_csv(ctr_by_type, file.path(OUT_DIR, "ctr_by_recommendation_type.csv"))
  p1 <- ggplot(ctr_by_type, aes(x = reorder(recommendation_type, ctr), y = ctr)) + geom_col(fill = viridis(1, option = "C")) + coord_flip() + scale_y_continuous(labels = scales::label_percent(accuracy = 0.1)) + labs(title = "CTR by recommendation type", x = "", y = "CTR") + theme_minimal()
  pos_ctr <- rec %>% filter(!is.na(position_in_list)) %>% group_by(position_in_list) %>% summarise(ctr = mean(was_clicked, na.rm = TRUE), n = n(), .groups = "drop")
  p2 <- ggplot(pos_ctr %>% filter(position_in_list <= 20), aes(x = position_in_list, y = ctr)) + geom_line() + geom_point() + scale_y_continuous(labels = scales::label_percent(accuracy = 0.1)) + labs(title = "CTR by position in list (top 20)", x = "Position", y = "CTR") + theme_minimal()
  p1 + p2
}
```

Interpretation: Include exact numeric comparisons and sample sizes below the figure when you knit.

## 4.3 Formal modeling: logistic regression and diagnostics

We fit a logistic regression for click likelihood controlling for algorithm_version, position, and time_of_day (if available). We show tidy table with OR and 95% CI and compute AUC.

```{r rec-logistic, cache=TRUE}
library(dplyr)
library(broom)
library(readr)
library(ggplot2)
library(purrr)
library(tidyr)

OUT_DIR <- if (exists("OUT_DIR")) OUT_DIR else "output"
dir.create(OUT_DIR, showWarnings = FALSE, recursive = TRUE)

if (nrow(rec) == 0) {
  message("Skipping rec-logistic: recommendation logs missing or empty")
} else {
  # Ensure was_clicked column and convert to numeric 0/1
  if (!"was_clicked" %in% names(rec)) stop("recommendation logs missing 'was_clicked' column")
  rec <- rec %>% mutate(
    was_clicked_flag = case_when(
      is.na(was_clicked) ~ NA_real_,
      is.logical(was_clicked) ~ as.numeric(as.integer(was_clicked)),
      is.numeric(was_clicked) ~ as.numeric(was_clicked),
      TRUE ~ as.numeric(as.integer(tolower(as.character(was_clicked)) %in% c("1","true","t","yes","y")))
    )
  )

  # Candidate predictors (update list if you have other contextual columns)
  candidate_predictors <- c("algorithm_version", "position_in_list", "time_of_day")
  available_predictors <- candidate_predictors[candidate_predictors %in% names(rec)]

  # Build modeling DF
  rec_model_df <- rec %>%
    select(all_of(c("was_clicked_flag", available_predictors))) %>%
    rename(was_clicked = was_clicked_flag) %>%
    mutate(across(where(is.character), ~ na_if(.x, "")))

  # Coerce types sensibly
  if ("algorithm_version" %in% available_predictors) rec_model_df <- rec_model_df %>% mutate(algorithm_version = as.factor(algorithm_version))
  if ("time_of_day" %in% available_predictors) rec_model_df <- rec_model_df %>% mutate(time_of_day = as.factor(time_of_day))
  if ("position_in_list" %in% available_predictors) rec_model_df <- rec_model_df %>% mutate(position_in_list = as.numeric(position_in_list))

  # Drop rows with missing response and required predictors
  req_cols <- c("was_clicked", available_predictors)
  if (length(available_predictors) > 0) {
    rec_model_df2 <- rec_model_df %>% drop_na(all_of(req_cols))
  } else {
    rec_model_df2 <- rec_model_df %>% drop_na("was_clicked")
  }

  if (nrow(rec_model_df2) == 0) {
    message("No complete rows for modeling after dropping NAs. Skipping model.")
  } else if (length(unique(rec_model_df2$was_clicked)) < 2) {
    message("Response 'was_clicked' has <2 classes after NA removal; cannot build model.")
  } else if (length(available_predictors) == 0) {
    message("No available predictors in rec. Skipping modeling (no X variables).")
  } else {
    # Fit logistic regression on cleaned data
    fmla <- as.formula(paste("was_clicked ~", paste(available_predictors, collapse = " + ")))
    glm1 <- glm(fmla, family = binomial(link = "logit"), data = rec_model_df2)

    # Tidy model results and save
    glm_tbl <- broom::tidy(glm1, conf.int = TRUE, exponentiate = TRUE) %>% mutate(across(where(is.numeric), ~round(.x, 4)))
    if (nrow(glm_tbl) > 0) {
      print(knitr::kable(glm_tbl, caption = "Logistic regression (exponentiated coefficients = OR)"))
      write_csv(glm_tbl, file.path(OUT_DIR, "rec_logistic_summary.csv"))
    }

    # Use model.frame to get the exact rows used by the model (alignment guaranteed)
    model_frame <- stats::model.frame(glm1)
    response_vec <- stats::model.response(model_frame)
    preds <- as.numeric(glm1$fitted.values)  # fitted on model_frame rows

    # Ensure response numeric 0/1
    response_num <- if (is.factor(response_vec)) {
      rn <- suppressWarnings(as.numeric(as.character(response_vec)))
      if (all(is.na(rn))) as.integer(response_vec) - 1 else as.integer(rn)
    } else {
      as.integer(response_vec)
    }

    # Persist preds & response_num to global env so downstream chunks can access them
    assign("preds", preds, envir = .GlobalEnv)
    assign("response_num", response_num, envir = .GlobalEnv)

    # Compute AUC/ROC if pROC available
    if (requireNamespace("pROC", quietly = TRUE)) {
      roc_obj <- pROC::roc(response_num, preds, quiet = TRUE)
      auc_val <- as.numeric(pROC::auc(roc_obj))
      cat("In-sample AUC:", round(auc_val, 4), "\n")
      # Save ROC plot
      png(file.path(OUT_DIR, "rec_model_roc.png"), width = 800, height = 600)
      plot(roc_obj, main = paste0("ROC curve (in-sample) — AUC = ", round(auc_val, 4)))
      dev.off()
    } else {
      message("pROC not installed; skipping ROC/AUC")
    }

    # Compute confusion / metrics at threshold = 0.5 and sweep
    get_cell_safe <- function(tab, r, c) {
      if (is.null(dim(tab))) return(0L)
      rn <- rownames(tab); cn <- colnames(tab)
      if (!is.null(rn) && !is.null(cn) && r %in% rn && c %in% cn) return(as.integer(tab[r, c]))
      return(0L)
    }
    threshold <- 0.5
    class_pred <- as.integer(preds >= threshold)
    cm_tab <- table(Predicted = as.character(class_pred), Actual = as.character(response_num))
    tp <- get_cell_safe(cm_tab, "1", "1"); tn <- get_cell_safe(cm_tab, "0", "0")
    fp <- get_cell_safe(cm_tab, "1", "0"); fn <- get_cell_safe(cm_tab, "0", "1")
    total <- tp + tn + fp + fn
    accuracy <- if (total > 0) (tp + tn) / total else NA_real_
    precision <- if ((tp + fp) > 0) tp / (tp + fp) else NA_real_
    recall <- if ((tp + fn) > 0) tp / (tp + fn) else NA_real_
    f1 <- if (!is.na(precision) && !is.na(recall) && (precision + recall) > 0) 2 * precision * recall / (precision + recall) else NA_real_

    cat("Confusion matrix at threshold =", threshold, ":\n")
    print(cm_tab)
    cat(sprintf("Total evaluated = %d\n", total))
    cat(sprintf("Accuracy = %.4f\nPrecision = %s\nRecall = %s\nF1 = %s\n",
                ifelse(is.na(accuracy), NA, round(accuracy, 4)),
                ifelse(is.na(precision), "NA", format(round(precision, 4), nsmall = 4)),
                ifelse(is.na(recall), "NA", format(round(recall, 4), nsmall = 4)),
                ifelse(is.na(f1), "NA", format(round(f1, 4), nsmall = 4))
    ))

    # Save metrics summary
    metrics_summary <- tibble::tibble(
      threshold = threshold,
      tp = tp, tn = tn, fp = fp, fn = fn,
      total = total,
      accuracy = accuracy,
      precision = precision,
      recall = recall,
      f1 = f1
    )
    write_csv(metrics_summary, file.path(OUT_DIR, "rec_model_metrics_threshold_0.5.csv"))

    # Threshold sweep
    thrs <- seq(0.01, 0.99, by = 0.01)
    sweep_df <- purrr::map_dfr(thrs, function(t) {
      cp <- as.integer(preds >= t)
      tab <- table(Predicted = as.character(cp), Actual = as.character(response_num))
      tp_ <- get_cell_safe(tab, "1", "1"); tn_ <- get_cell_safe(tab, "0", "0")
      fp_ <- get_cell_safe(tab, "1", "0"); fn_ <- get_cell_safe(tab, "0", "1")
      total_ <- tp_ + tn_ + fp_ + fn_
      acc_ <- if (total_ > 0) (tp_ + tn_) / total_ else NA_real_
      prec_ <- if ((tp_ + fp_) > 0) tp_ / (tp_ + fp_) else NA_real_
      rec_ <- if ((tp_ + fn_) > 0) tp_ / (tp_ + fn_) else NA_real_
      f1_ <- if (!is.na(prec_) && !is.na(rec_) && (prec_ + rec_) > 0) 2 * prec_ * rec_ / (prec_ + rec_) else NA_real_
      tibble::tibble(threshold = t, tp = tp_, tn = tn_, fp = fp_, fn = fn_, total = total_, accuracy = acc_, precision = prec_, recall = rec_, f1 = f1_)
    }, .id = NULL)
    write_csv(sweep_df, file.path(OUT_DIR, "rec_model_threshold_sweep_metrics.csv"))

    # Save some plots (PR/Accuracy)
    if (requireNamespace("ggplot2", quietly = TRUE)) {
      p_pr <- ggplot(sweep_df, aes(x = threshold)) +
        geom_line(aes(y = precision, color = "Precision")) +
        geom_line(aes(y = recall, color = "Recall")) +
        labs(title = "Precision and Recall vs Threshold", y = "Metric", x = "Threshold") +
        scale_color_manual(values = c("Precision" = "#1b9e77", "Recall" = "#d95f02")) +
        theme_minimal()
      p_acc <- ggplot(sweep_df, aes(x = threshold, y = accuracy)) +
        geom_line(color = viridis::viridis(1)) +
        labs(title = "Accuracy vs Threshold", y = "Accuracy", x = "Threshold") + theme_minimal()
      ggsave(file.path(OUT_DIR, "rec_threshold_pr.png"), plot = p_pr, width = 8, height = 4, dpi = 200)
      ggsave(file.path(OUT_DIR, "rec_threshold_accuracy.png"), plot = p_acc, width = 8, height = 4, dpi = 200)
      print(p_pr + p_acc)
    }

    # Basic CV AUC if enough data
    if (nrow(rec_model_df2) >= 200 && requireNamespace("pROC", quietly = TRUE)) {
      set.seed(123)
      k <- 5
      nrows_cv <- nrow(rec_model_df2)
      folds <- sample(rep(1:k, length.out = nrows_cv))
      aucs <- c()
      for (fold in 1:k) {
        train_idx <- which(folds != fold)
        test_idx <- which(folds == fold)
        train_df <- rec_model_df2[train_idx, , drop = FALSE]
        test_df <- rec_model_df2[test_idx, , drop = FALSE]
        if (length(unique(train_df$was_clicked)) < 2) next
        fit_cv <- glm(fmla, family = binomial(link = "logit"), data = train_df)
        preds_cv <- predict(fit_cv, newdata = test_df, type = "response")
        ok <- !(is.na(preds_cv) | is.na(test_df$was_clicked))
        if (sum(ok) > 10) {
          roc_cv <- tryCatch(pROC::roc(test_df$was_clicked[ok], preds_cv[ok], quiet = TRUE), error = function(e) NULL)
          if (!is.null(roc_cv)) aucs <- c(aucs, as.numeric(pROC::auc(roc_cv)))
        }
      }
      if (length(aucs) > 0) {
        cat("Cross-validated AUC (", k, "-fold) mean (sd): ", round(mean(aucs), 4), " (", round(sd(aucs), 4), ")\n", sep = "")
        write_csv(tibble::tibble(fold = seq_along(aucs), auc = aucs), file.path(OUT_DIR, "rec_model_cv_auc_by_fold.csv"))
      } else {
        message("CV AUC computation produced no valid ROC folds.")
      }
    } else {
      message("Skipping CV AUC (nrows < 200 or pROC not available).")
    }

    cat("\nInterpretation:\n")
    cat("- In-sample AUC reported above. If close to 0.5, model has little predictive signal.\n")
    cat("- Use threshold sweep CSV to analyze tradeoffs and saved plots for visual inspection.\n")
    cat("- preds and response_num have been assigned into the global environment for downstream chunks.\n")
  } # end model run
} # end else rec present
```

---

# 5. Cross-dataset analyses & formal test: Do recommended titles get more engagement?

We compare progress for sessions of recommended vs not recommended titles using both a t-test and a bootstrap if infer is available.

```{r rec-impact, cache=TRUE}
# Link watch sessions to recommendation events using user_id + movie_id + time window.
# Defensive: prefers recommendation_date and watch_date_parsed, robust parsing, writes diagnostics to output/.
library(dplyr)
library(lubridate)
library(purrr)
library(ggplot2)
library(viridis)
library(readr)
library(broom)

# Parameters (tune as needed)
window_hours <- 24   # conservative default: 24 hours
min_group_n  <- 5    # minimum rows per group to run tests
OUT_DIR <- if(exists("OUT_DIR")) OUT_DIR else "output"
dir.create(OUT_DIR, showWarnings = FALSE, recursive = TRUE)

# Robust vector parse function
safe_parse_datetime_vec <- function(v) {
  if (inherits(v, "POSIXt")) return(as_datetime(v))
  if (inherits(v, "Date")) return(as_datetime(as_date(v)))
  v_chr <- as.character(v)
  out <- rep(NA_real_, length(v_chr))

  # try as.POSIXct (ISO-like)
  pos <- suppressWarnings(as.POSIXct(v_chr, tz = "UTC"))
  ok <- !is.na(pos)
  out[ok] <- as.numeric(pos[ok])

  # remaining: try lubridate parse_date_time safely
  rem <- which(is.na(out) & !is.na(v_chr) & v_chr != "")
  if (length(rem) > 0) {
    parsed <- tryCatch(
      parse_date_time(v_chr[rem],
                      orders = c("ymd HMS","ymd HM","ymd","mdy HMS","mdy HM","mdy",
                                 "dmy HMS","dmy HM","dmy","Y-m-d H:M:S","ISO8601"),
                      tz = "UTC"),
      error = function(e) rep(NA_real_, length(rem))
    )
    out[rem] <- as.numeric(parsed)
  }

  # numeric epoch fallback
  num <- suppressWarnings(as.numeric(v_chr))
  is_num <- !is.na(num)
  if (any(is_num)) {
    vals <- num[is_num]
    vals[vals > 1e12] <- vals[vals > 1e12] / 1000   # ms -> s if necessary
    out[is_num] <- vals
  }

  as_datetime(out, tz = "UTC")
}

# Ensure data present
if (nrow(rec) == 0 || nrow(watch) == 0) {
  message("Skipping rec-impact: rec or watch empty")
} else if (!("movie_id" %in% names(rec)) || !("movie_id" %in% names(watch))) {
  message("Skipping rec-impact: movie_id missing in rec or watch")
} else {

  # Candidate names (include the actual column you have: recommendation_date)
  rec_time_candidates <- c("recommendation_date","recommendation_time","recommendation_timestamp","rec_time","created_at","timestamp","event_time","time","impression_time")
  watch_time_candidates <- c("watch_date_parsed","watch_datetime","watch_date","watch_time","timestamp","created_at")

  rec_time_col <- intersect(names(rec), rec_time_candidates) %>% first()
  watch_time_col <- intersect(names(watch), watch_time_candidates) %>% first()

  if (is.null(watch_time_col)) stop("No watch time column found (expected watch_date_parsed or watch_date). Add a parsed watch timestamp.")
  message("Using watch time column: ", watch_time_col)

  # Parse watch times
  watch <- watch %>%
    mutate(.watch_time_raw = .data[[watch_time_col]],
           .watch_time = safe_parse_datetime_vec(.watch_time_raw))

  if (is.null(rec_time_col)) {
    message("No recommendation timestamp column found in rec. Falling back to 'ever recommended' marker (movie-level).")
    rec_movies <- rec %>% filter(!is.na(movie_id)) %>% distinct(movie_id)
    watch2 <- watch %>% mutate(min_pos_diff_hours = Inf,
                               was_recommended_movie = movie_id %in% rec_movies$movie_id)
  } else {
    message("Using recommendation time column: ", rec_time_col)
    # Parse rec times
    rec <- rec %>%
      mutate(.rec_time_raw = .data[[rec_time_col]],
             .rec_time = safe_parse_datetime_vec(.rec_time_raw))

    # If rec lacks user_id, we cannot make user-level links; fall back to movie-level
    if (!"user_id" %in% names(rec)) {
      message("rec lacks user_id; falling back to movie-level 'ever recommended' marker.")
      rec_movies <- rec %>% filter(!is.na(movie_id)) %>% distinct(movie_id)
      watch2 <- watch %>% mutate(min_pos_diff_hours = Inf,
                                 was_recommended_movie = movie_id %in% rec_movies$movie_id)
    } else {
      # prepare rec events and group rec times by user+movie
      rec_events <- rec %>% filter(!is.na(user_id) & !is.na(movie_id) & !is.na(.rec_time)) %>%
        select(user_id, movie_id, .rec_time)

      rec_times_tbl <- rec_events %>%
        group_by(user_id, movie_id) %>%
        summarise(rec_times = list(.rec_time), .groups = "drop")

      # join rec_times onto watch
      watch2 <- watch %>%
        left_join(rec_times_tbl, by = c("user_id", "movie_id"))

      # compute min positive difference (watch_time - rec_time) in hours
      watch2 <- watch2 %>%
        mutate(min_pos_diff_hours = map2_dbl(.watch_time, rec_times, function(wt, rts) {
          if (is.na(wt) || is.null(rts) || length(rts) == 0) return(Inf)
          diffs <- as.numeric(difftime(wt, rts, units = "hours"))
          pos <- diffs[!is.na(diffs) & diffs >= 0]
          if (length(pos) == 0) return(Inf)
          min(pos, na.rm = TRUE)
        }),
        was_recommended_movie = if_else(min_pos_diff_hours <= window_hours, TRUE, FALSE))
    }
  }

  # Diagnostics: counts, quantiles, sample preview
  cat("\nCount by was_recommended_movie:\n")
  print(table(watch2$was_recommended_movie, useNA = "ifany"))

  if ("min_pos_diff_hours" %in% names(watch2)) {
    cat("\nSummary of min_pos_diff_hours (Inf = no prior recommendation):\n")
    print(summary(watch2$min_pos_diff_hours))
    cat("\nQuantiles (hours):\n")
    print(quantile(watch2$min_pos_diff_hours, probs = c(0,0.01,0.05,0.1,0.5,0.9,0.99), na.rm = TRUE))
    cat("\nCount Inf (no prior rec): ", sum(is.infinite(watch2$min_pos_diff_hours) | is.na(watch2$min_pos_diff_hours)), "\n")
    cat("Count within window (<= ", window_hours, "h): ", sum(watch2$min_pos_diff_hours <= window_hours, na.rm = TRUE), "\n")
  }

  # Save a small sample including the first rec time (for debugging)
  if ("rec_times" %in% names(watch2)) {
    watch2 <- watch2 %>%
      mutate(rec_time_first = map_chr(rec_times, ~ if (is.null(.x) || length(.x) == 0) NA_character_ else as.character(.x[[1]])))
    write_csv(watch2 %>% select(session_id = any_of("session_id"), user_id, movie_id, .watch_time_raw, .watch_time, rec_time_first, min_pos_diff_hours, was_recommended_movie, progress_percentage) %>% slice_head(n = 2000),
              file.path(OUT_DIR, "watch_recommendation_link_sample_fixed.csv"))
  } else {
    write_csv(watch2 %>% select(session_id = any_of("session_id"), user_id, movie_id, .watch_time_raw, .watch_time, min_pos_diff_hours, was_recommended_movie, progress_percentage) %>% slice_head(n = 2000),
              file.path(OUT_DIR, "watch_recommendation_link_sample_fixed.csv"))
  }

  # Compare progress by group (if progress exists)
  if (!"progress_percentage" %in% names(watch2)) {
    message("progress_percentage not present in watch; cannot compare progress.")
  } else {
    rec_compare <- watch2 %>%
      filter(!is.na(progress_percentage)) %>%
      group_by(was_recommended_movie) %>%
      summarise(avg_progress = mean(progress_percentage, na.rm = TRUE),
                sd_progress = sd(progress_percentage, na.rm = TRUE),
                n = n(),
                median_progress = median(progress_percentage, na.rm = TRUE),
                .groups = "drop") %>%
      arrange(desc(n))
    print(rec_compare)
    write_csv(rec_compare, file.path(OUT_DIR, "recommended_vs_not_progress_summary_by_window_fixed.csv"))

    groups_ok <- rec_compare %>% filter(!is.na(was_recommended_movie) & n >= min_group_n)
    if (nrow(groups_ok) < 2) {
      message("Skipping statistical tests: need at least two groups with >=", min_group_n, "observations each. Group counts:")
      print(rec_compare)
    } else {
      test_df <- watch2 %>% filter(!is.na(progress_percentage) & !is.na(was_recommended_movie)) %>%
        mutate(was_recommended_movie = as.factor(was_recommended_movie))
      t_res <- tryCatch(t.test(progress_percentage ~ was_recommended_movie, data = test_df, var.equal = FALSE), error = function(e) e)
      if (inherits(t_res, "htest")) {
        print(t_res)
        write_csv(broom::tidy(t_res), file.path(OUT_DIR, "recommended_vs_not_ttest_by_window_fixed.csv"))
      } else {
        message("t.test failed: ", t_res)
      }
      if (requireNamespace("infer", quietly = TRUE)) {
        boot_diff <- test_df %>% specify(progress_percentage ~ was_recommended_movie) %>% generate(reps = 2000, type = "bootstrap") %>% calculate(stat = "diff in means")
        write_csv(as_tibble(boot_diff), file.path(OUT_DIR, "bootstrap_diff_in_means_by_window_fixed.csv"))
        cat("Bootstrap diff in means: median = ", median(boot_diff$stat, na.rm = TRUE), "\n")
      }
      p <- ggplot(test_df, aes(x = was_recommended_movie, y = progress_percentage, fill = was_recommended_movie)) +
        geom_boxplot(outlier.shape = NA, alpha = 0.6) + geom_jitter(width = 0.15, alpha = 0.2) +
        scale_fill_viridis_d(option = "C") + theme_minimal() +
        labs(title = paste0("Progress: recommended vs not (", window_hours, "h window)"), x = "Was recommended (within window)", y = "Progress (%)")
      print(p)
      ggsave(file.path(OUT_DIR, paste0("recommended_vs_not_progress_boxplot_window_", window_hours, "h_fixed.png")), plot = p, width = 8, height = 5, dpi = 200)
    }
  }
} 
```

Interpretation: After running, inspect rec_compare and t_res (or bootstrap) for effect size, p-value and CI. Add a short sentence here describing whether the difference is statistically significant and the practical magnitude.

---

# 6. Correlation & multivariate patterns (movies numeric features)

```{r movies-corr, cache=TRUE}
if(nrow(movies)>0){
  num_movies <- movies %>% select(where(is.numeric))
  if(ncol(num_movies) >= 2){
    nm <- num_movies %>% select_if(~sum(!is.na(.)) > 0)
    if(ncol(nm) >= 2){
      M <- cor(na.omit(nm), use = "complete.obs")
      corrplot(M, method = "color", tl.cex = 0.7, number.cex = 0.6)
      # save matrix
      write_csv(as.data.frame(M), file.path(OUT_DIR, "movies_numeric_correlations.csv"))
    } else cat("Not enough numeric columns after filtering\n")
  } else cat("Not enough numeric columns for correlation matrix\n")
}
```

---

# Appendix — Save artifacts & reproducibility

```{r save-artifacts, eval=TRUE}
# Examples saved above in each section; ensure at least one plot saved
if(exists("p1")) ggsave(file.path(OUT_DIR, "imdb_rating_density.png"), plot = p1, width = 8, height = 4, dpi = 300)
if(exists("p_topviews")) ggsave(file.path(OUT_DIR, "top_20_movies_by_views.png"), plot = p_topviews, width = 10, height = 6, dpi = 300)
# session info
writeLines(capture.output(sessionInfo()), file.path(OUT_DIR, "sessionInfo.txt"))
```

Data provenance & methods
------------------------
- These cleaned CSVs are assumed to be produced by the upstream cleaning notebooks in this repo (movies_data_cleaning.rmd, users_data_cleaning.rmd, watch_history_data_cleaning.rmd, recommendation_logs_data_cleaning.rmd). Any decisions (e.g., how progress_percentage or watch_duration_minutes were computed) should be documented in those cleaning scripts; include a short summary here if there were non-trivial imputations.

Final checklist for graders
--------------------------
- Executive summary: included (top).
- Formal tests/models: t-test/bootstrap + logistic regression + AUC included.
- Interpretations: placeholders are present — run the report to auto-fill numeric outputs and then paste the numeric interpretation sentences under each major plot (recommended: 1–3 lines each).
- Artifacts: output/ contains saved CSVs, PNGs, and sessionInfo.txt.

Session info (for reproducibility)
```{r session-info, echo=FALSE}
sessionInfo()
```

